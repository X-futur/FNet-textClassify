{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89e0e6f5",
   "metadata": {},
   "source": [
    "# 使用 Transformer 进行文本分类\n",
    "https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5060f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28837dfb",
   "metadata": {},
   "source": [
    "## 将 Transformer 块实现为层\n",
    "参数：embed_dim: 输入向量的维度(嵌入维度)，num_heads: 多头注意力机制的头数，ff_dim: 前馈神经网络中间层的维度，rate: dropout率，默认为0.1<br><br>\n",
    "组件:self.att: 多头注意力层(Multi-Head Attention)。self.ffn: 前馈神经网络(Feed Forward Network)，两个全连接层。self.layernorm1 和 self.layernorm2: 两个层归一化(Layer Normalization)。self.dropout1 和 self.dropout2: 两个dropout层<br><br>\n",
    "实现遵循了原始Transformer论文的标准结构：多头自注意力层，残差连接 + 层归一化，前馈神经网络，残差连接 + 层归一化<br><br>\n",
    "这种结构使得模型能够:通过自注意力机制捕获序列中元素间的长距离依赖关系，通过残差连接缓解深层网络的梯度消失问题，通过层归一化稳定训练过程，通过dropout防止过拟合<br><br>\n",
    "<strong>ffn</strong> 是 Feed-Forward Network（前馈神经网络）的缩写，ffn 通常是一个 两层的全连接神经网络，用于对自注意力层（MultiHeadAttention）的输出进行非线性变换，增强模型的表达能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31ab8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现了Transformer块（编码器架构）\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        # 多头自注意力\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # 层归一化\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # 前馈神经网络\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # 层归一化\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        # 丢弃层\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    # 前向传播方法\n",
    "    def call(self, inputs):\n",
    "        # 自注意力机制\n",
    "        attn_output = self.att(inputs, inputs)  # 计算输入序列的自注意力\n",
    "        attn_output = self.dropout1(attn_output)  # 正则化\n",
    "        out1 = self.layernorm1(inputs + attn_output)  # 第一次残差连接和层归一化\n",
    "\n",
    "        # 前馈神经网络\n",
    "        ffn_output = self.ffn(out1) # 通过一个两层的ffn处理数据\n",
    "        ffn_output = self.dropout2(ffn_output)  # 正则化\n",
    "        return self.layernorm2(out1 + ffn_output)  # 残差连接层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be30e15c",
   "metadata": {},
   "source": [
    "## 实现嵌入层\n",
    "两个单独的嵌入层，一个用于token，一个用于token索引（位置）。<br><br>\n",
    "参数：maxlen: 序列的最大长度，vocab_size: 词汇表大小，embed_dim: 嵌入维度<br><br>\n",
    "初始化了两个嵌入层：token_emb: 用于 token 的嵌入，将词汇 ID 映射为向量。pos_emb: 用于位置的嵌入，将位置索引映射为向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1951dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # 获取输入序列的实际长度\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        # 生成位置序列 [0, 1, 2, ..., maxlen-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        # 获取位置嵌入\n",
    "        positions = self.pos_emb(positions)\n",
    "        # 获取 token 嵌入\n",
    "        x = self.token_emb(x)\n",
    "        # 将两者相加作为最终输出\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeda1f6",
   "metadata": {},
   "source": [
    "## 下载并准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a13085c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "# 方案一 IMDB 电影评论数据集（IMDB Movie Reviews Sentiment Dataset）\n",
    "# 它是 Keras 内置的一个经典 NLP 数据集，包含 50,000 条带有正面/负面标签的影评，常用于文本分类任务。\n",
    "\n",
    "vocab_size = 50000  # Only consider the top 20k words\n",
    "maxlen = 300  # Only consider the first 200 words of each movie review\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce7dddd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 文本编码（需先构建词汇表，或用 Hugging Face Tokenizer）\u001b[39;00m\n\u001b[0;32m     14\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mTokenizer(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_on_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m x_train \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     17\u001b[0m x_val \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(val_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\miniconda\\envs\\DLHomework\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\text.py:143\u001b[0m, in \u001b[0;36mTokenizer.fit_on_texts\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m seq:\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_counts:\n\u001b[1;32m--> 143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_counts[w] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_counts[w] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 方案二 Amazon Reviews\n",
    "# 数据量：数千万条商品评论（比 IMDB 大得多）。\n",
    "# 获取方式：通过 Amazon Product Data 或 Hugging Face 的 amazon_polarity 数据集。\n",
    "\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"amazon_polarity\")\n",
    "train_data, val_data = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "# 文本编码（需先构建词汇表，或用 Hugging Face Tokenizer）\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_data[\"content\"])\n",
    "x_train = tokenizer.texts_to_sequences(train_data[\"content\"])\n",
    "x_val = tokenizer.texts_to_sequences(val_data[\"content\"])\n",
    "\n",
    "# 填充序列\n",
    "maxlen = 200\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "# 标签\n",
    "y_train = train_data[\"label\"]\n",
    "y_val = val_data[\"label\"]\n",
    "print(\"Unique labels:\", np.unique(y_train))  # 应输出 [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef0c22",
   "metadata": {},
   "source": [
    "## 使用 transformer 层创建分类器模型\n",
    "Transformer 层为输入序列的每个时间步输出一个向量。 在这里，我们取所有时间步长的均值，并且 在其上使用前馈网络对文本进行分类。<br><br>\n",
    "参数：embed_dim: 控制嵌入向量的大小。num_heads: 决定多头注意力的并行注意力机制数量。ff_dim: Transformer内部前馈网络的维度<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "067bdecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # 每个token的嵌入维度\n",
    "num_heads = 2  # 注意力头的数量\n",
    "ff_dim = 32  # Transformer前馈网络的隐藏层大小\n",
    "\n",
    "# 输入层：定义模型输入，形状为 (maxlen,)，即固定长度的序列\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "\n",
    "# 嵌入层\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)  # 使用之前定义的 TokenAndPositionEmbedding 层\n",
    "x = embedding_layer(inputs)  # 将输入的 token IDs 转换为嵌入向量并添加位置信息\n",
    "\n",
    "#  Transformer 块\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)  # 应用 Transformer 块处理序列数据\n",
    "x = transformer_block(x) \n",
    "\n",
    "# 全局池化\n",
    "x = layers.GlobalAveragePooling1D()(x)  # 对序列维度进行平均池化，将变长序列转换为固定长度的表示（维度=embed_dim）\n",
    "\n",
    "# 分类头部\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "# 模型定义\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04768eef",
   "metadata": {},
   "source": [
    "## 训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd1768ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 110ms/step - accuracy: 0.6649 - loss: 0.5586 - val_accuracy: 0.8840 - val_loss: 0.2817\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 101ms/step - accuracy: 0.9425 - loss: 0.1667 - val_accuracy: 0.8803 - val_loss: 0.3039\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 92ms/step - accuracy: 0.9707 - loss: 0.0886 - val_accuracy: 0.8683 - val_loss: 0.4040\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 90ms/step - accuracy: 0.9882 - loss: 0.0449 - val_accuracy: 0.8510 - val_loss: 0.6051\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 97ms/step - accuracy: 0.9912 - loss: 0.0307 - val_accuracy: 0.8486 - val_loss: 0.7572\n"
     ]
    }
   ],
   "source": [
    "# 方案一\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7f03f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[   0    0    0 ... 2730    5  329]\n [   0    0    0 ...  174  158 2700]\n [   0    0    0 ...    6  402  266]\n ...\n [   0    0    0 ...   45  690 1255]\n [   0    0    0 ...    7    1   35]\n [   0    0    0 ...   18 4513 2101]] (of type <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 适用于整数标签\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_val, y_val)\n",
      "File \u001b[1;32md:\\miniconda\\envs\\DLHomework\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\miniconda\\envs\\DLHomework\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:125\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=[[   0    0    0 ... 2730    5  329]\n [   0    0    0 ...  174  158 2700]\n [   0    0    0 ...    6  402  266]\n ...\n [   0    0    0 ...   45  690 1255]\n [   0    0    0 ...    7    1   35]\n [   0    0    0 ...   18 4513 2101]] (of type <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "# 方案二 使用Amazon Review数据集\n",
    "# 编译模型\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # 适用于整数标签\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=2,\n",
    "    validation_data=(x_val, y_val)\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLHomework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
